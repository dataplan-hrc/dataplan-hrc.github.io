<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Anticipate & Collab: Data-driven Task Anticipation and
        Knowledge-driven Planning for Human-robot Collaboration">
  <meta name="keywords" content="Task Anticipation, Human Robot Collaboration, Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Anticipate & Collab: Data-driven Task Anticipation and
    Knowledge-driven Planning for Human-robot Collaboration</title>
    <style>
      pre {
        white-space: pre-wrap; /* Preserve line breaks */
        font-family: 'Courier New', Courier, monospace; /* Use monospaced font */
        background-color: #f4f4f4; /* Light gray background */
        padding: 1px; /* Add padding */
        border: 1px solid #ccc; /* Add border */
        border-radius: 1px; /* Add border radius */
      }
      .vscode-theme {
        font-family: 'Consolas', 'Courier New', monospace;
        background-color: #1e1e1e;
        color: #d4d4d4;
        padding: 20px;
        border-radius: 5px;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
      }      
    </style>    

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-P16Y10FFHE');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://dataplan-hrc.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://robotics.iiit.ac.in/research/">
            RRC Research
          </a>
          <a class="navbar-item" href="https://robotics.iiit.ac.in/publications/">
            Other Publications
          </a>
          
          <!-- <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Anticipate & Collab: Data-driven Task Anticipation and             Knowledge-driven Planning for Human-robot Collaboration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sssshivvvv/">Shivam Singh</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://kart1802.github.io">Karthik Swaminathan</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://raraghavarora.github.io">Raghav Arora</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://theramandeep04.github.io/">Ramandeep Singh</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://adata111.github.io/">Ahana Datta</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dataplan-hrc.github.io">Dipanjan Das</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://snehasisb.github.io/">Snehasis Banerjee</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homepages.inf.ed.ac.uk/msridhar/">Mohan Sridharan</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">Madhava Krishna</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Robotics Research Center, IIIT Hyderabad, India; </span>
            <span class="author-block"><sup>2</sup>TCS Research, Tata Consultancy Services, India; </span>
            <span class="author-block"><sup>3</sup>School of Informatics, University of Edinburgh, UK</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="./assets/AnticipateNCollab_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://dataplan-hrc.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary Material</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.03587"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/QW5VCDIgXus"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://dataplan-hrc.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://dataplan-hrc.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            An agent assisting humans in daily living activities can collaborate more effectively by anticipating upcoming tasks. Data-driven methods represent the state of the art in task anticipation, planning, and related problems, but these methods are resource-hungry and opaque. Our prior work introduced a proof of concept framework that used an LLM to anticipate 3 high-level tasks that served as goals for a classical planning system that computed a sequence of low-level actions for the agent to achieve these goals. This paper describes DaTAPlan, our framework that significantly extends our prior work toward human-robot collaboration. Specifically, DaTAPlan's planner computes actions for an agent and a human to collaboratively and jointly achieve the tasks anticipated by the LLM, and the agent automatically adapts to unexpected changes in human action outcomes and preferences. We evaluate DaTAPlan's capabilities in a realistic simulation environment, demonstrating accurate task anticipation, effective human-robot collaboration, and the ability to adapt to unexpected changes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework</h2>
        <div class="columns is-centered has-text-centered">
          <div>
            <div class="publication-video">
              <img width=200% src="./static/images/HRC_Pipeline.png" alt="Italian Trulli">
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Our Framework consists of four parts:<br>
            <ul>
              <li>In the section (a) of the above figure, our system uses LLM models to generate high level tasks as an output of anticipation. The LLM model is provided with 3 key information namely a Task List (consisting of all the possible tasks that the human can perform in the environment), User Preference (a set of tasks that can vary based on the type of user/human in the environment), Scene Description (highlighting the situation in the environment with object states and types). The system has also accommodated any failure cases (or rather Hallucinations caused by the LLM). A situation where any object is not present in the environment and an alternative is needed has been taken care of by the LLM Model itself <em>(Please view the "Resource Availability" Section in the Supplementary Material)</em> </li>

              <li>In section (b), the tasks generated by the LLM are mapped to the Planning Domain Definition Language (PDDL) problem description. We used <em>lama</em> configuration provided the <em>Fast-Downward</em> system. The Planner using the domain and problem description files generates a plan with the primary focus on reducing the execution cost. <br> <b>NOTE: The cost for all the type of actions can be viewed in the Supplementary material.</b> </li>

              <li>In the section (c), we show an example of plan generated by the planner. The generated plan is the expectation that the robot has regarding the distribution of actions in a collaborative setup in jointly completing the task. .  </li>

              <li>In the section (d) of the above figure, our system finally integrates the output of Task Planning with the actors present in the Coppeliasim Simulation Environment. We are faced with three challenges: <ol> <li> <b>Reprompting:</b> An immediate change in the course of action when a situation is changed by the human. For Example: When the tasks related to <em>"preparation of breakfast"</em> is being executed and the human immediately needs to attend the meeting, then subsequently the <em>"setting up an office table"</em> task would be performed by both the actors catering to the human's immediate requirement. </li>  </li>
              <li> <b>Replanning:</b> There can be situations where there is a slight deviation in the expected robot action plan. An adaptive response is generated and an updated plan with the correction is generated. <br> <b>NOTE: An example of replanning can be seen in the video below.</b> </li> </li>
              <li> <b>Collision Avoidance:</b> At the level of Motion Planning, the actors are tracked and the response to a possible future collision compels the robot to stop when transiting to it's goal position. <br> <b>NOTE: Checkout the video below</b></li> </ol> </li>
            </ul>

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Video</h2>
          <p>Below is the PDDL problem file goal:</p>
          <p >
            <pre><code>(goal: 
  (and
    (boiled boiled_egg)
    (food_served boiled_egg plate_2
    dining_table kitchen)
    (prepared_clothes office_clothes
    ironing_board livingroom)
    (charged cellphone)
  )
) </code></pre>
          </p>
          <div>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pipeline_video.mp4"
            type="video/mp4">
          </video>
        </div>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Plan</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              <br>
              <img width=200% src="./static/images/Pipeline_plan.png" alt="Italian Trulli">
            </p>
            <!-- <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video> -->
          </div>

        </div>
      </div>
    </div>



<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">More Examples of Adaptation...</h2>
        <div class="columns is-centered has-text-centered">
          <div>
            <div class="publication-video">
              <img width=200% src="./static/images/HRC_Pipeline.png" alt="Italian Trulli">
            </div>
          </div>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    Paper video. -->
  <!-- </div>
</section> -->


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Video</h2>
          <p>
            This video showcases the execution and completion of tasks generated by the LLM and PDDL stack in a household scenario highlighting the collaborative efforts of human and the robot in the environment
          </p>
          <div>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pipeline_video.mp4"
            type="video/mp4">
          </video>
        </div>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Plan</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              <img width=200% src="./static/images/Pipeline_plan.png" alt="Italian Trulli">
            </p> -->
            <!-- <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video> -->
          <!-- </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      <!-- </div>
    </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->
<!-- 
  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{singh2024anticipate,
      title={Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration}, 
      author={Shivam Singh and Karthik Swaminathan and Raghav Arora and Ramandeep Singh and Ahana Datta and Dipanjan Das and Snehasis Banerjee and Mohan Sridharan and Madhava Krishna},
      year={2024},
      eprint={2404.03587},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
